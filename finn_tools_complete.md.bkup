# Financial Planning & Analysis (FP&A) AI Agent Tools Documentation

This documentation provides comprehensive information about available tools for FP&A AI agents, organized by category to enable optimal tool selection for financial analysis tasks.

## 1. Basic Math and Aggregation

### SUM

**Purpose:** Add up a range of numbers using Decimal precision for financial accuracy.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array or range of numeric values (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Sum of all values

**Use Cases:**
- Calculate total revenue, expenses, or cash flows
- Sum financial metrics across periods or categories
- Aggregate budget line items
- When you need to "add up", "total", "sum", or "aggregate" financial data

**Example:**
```python
SUM(ctx, [1, 2, 3, 4, 5])  # Returns Decimal('15')
SUM(ctx, "revenue_data.parquet")  # Sum from file
```

### AVERAGE

**Purpose:** Calculate the mean of a dataset using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array or range of numeric values (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Mean of all values

**Use Cases:**
- Calculate average revenue per customer
- Find mean expense amounts
- Determine typical performance metrics
- When you need to find "average", "mean", or "typical" values

**Example:**
```python
AVERAGE(ctx, [10, 20, 30])  # Returns Decimal('20')
```

### MIN

**Purpose:** Identify the smallest number in a dataset using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array or range of numeric values (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Minimum value

**Use Cases:**
- Find lowest cost, price, or expense
- Identify minimum performance thresholds
- Determine floor values for budgeting
- When you need "minimum", "lowest", "smallest", or "floor" values

**Example:**
```python
MIN(ctx, [10, 5, 20, 3])  # Returns Decimal('3')
```

### MAX

**Purpose:** Identify the largest number in a dataset using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array or range of numeric values (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Maximum value

**Use Cases:**
- Find highest revenue, profit, or performance
- Identify peak values for capacity planning
- Determine ceiling values for budgets
- When you need "maximum", "highest", "largest", or "peak" values

**Example:**
```python
MAX(ctx, [10, 5, 20, 3])  # Returns Decimal('20')
```

### PRODUCT

**Purpose:** Multiply values together using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array or range of numeric values (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Product of all values

**Use Cases:**
- Calculate compound growth rates
- Multiply price by quantity calculations
- Compute cumulative factors
- When you need to "multiply", "compound", or calculate "products"

**Example:**
```python
PRODUCT(ctx, [2, 3, 4])  # Returns Decimal('24')
```

### MEDIAN

**Purpose:** Calculate the middle value of a dataset using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Median value

**Use Cases:**
- Find typical values when outliers exist
- Analyze salary or compensation distributions
- Determine middle performance metrics
- When you need "median", "middle", or "50th percentile" values

**Example:**
```python
MEDIAN(ctx, [1, 2, 3, 4, 5])  # Returns Decimal('3')
```

### MODE

**Purpose:** Find the most frequently occurring value using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal or list of Decimals - Most frequent value(s)

**Use Cases:**
- Identify most common transaction amounts
- Find typical order sizes or quantities
- Analyze recurring expense patterns
- When you need "most common", "frequent", or "typical" values

**Example:**
```python
MODE(ctx, [1, 2, 2, 3, 3, 3])  # Returns Decimal('3')
```

### PERCENTILE

**Purpose:** Calculate specific percentiles using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `percentile_value`: Percentile value (0-1)

**Returns:** Decimal - Percentile value

**Use Cases:**
- Risk analysis (95th percentile for VaR)
- Performance benchmarking
- Outlier detection and analysis
- When you need "percentile", "quartile", or "quantile" analysis

**Example:**
```python
PERCENTILE(ctx, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], percentile_value=0.75)  # Returns Decimal('7.75')
```

### POWER

**Purpose:** Raise numbers to a power using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of base numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `power`: Exponent (single value applied to all numbers)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Results of values^power for each value

**Use Cases:**
- Calculate compound interest (1.05^years)
- Growth rate calculations
- Risk modeling with exponential functions
- When you need "power", "exponent", or "compound" calculations

**Example:**
```python
POWER(ctx, [2, 3, 4], power=2)  # Returns [Decimal('4'), Decimal('9'), Decimal('16')]
```

### SQRT

**Purpose:** Calculate square root using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Square roots of all values

**Use Cases:**
- Volatility calculations (standard deviation)
- Risk metrics calculations
- Geometric calculations for financial modeling
- When you need "square root", "volatility", or "standard deviation" calculations

**Example:**
```python
SQRT(ctx, [25, 16, 9])  # Returns [Decimal('5'), Decimal('4'), Decimal('3')]
```

### EXP

**Purpose:** Calculate e^x using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of exponents (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - e^values for each value

**Use Cases:**
- Continuous compounding calculations
- Exponential growth modeling
- Option pricing models (Black-Scholes)
- When you need "exponential", "continuous compounding", or "e^x" calculations

**Example:**
```python
EXP(ctx, [1, 2, 3])  # Returns exponential values
```

### LN

**Purpose:** Calculate natural logarithm using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Natural logarithms of all values

**Use Cases:**
- Calculate continuously compounded returns
- Log-normal distribution modeling
- Growth rate transformations
- When you need "natural log", "ln", or "logarithmic" transformations

**Example:**
```python
LN(ctx, [2.718281828459045, 1, 10])  # Returns natural log values
```

### LOG

**Purpose:** Calculate logarithm with specified base using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `base`: Base of logarithm (optional, defaults to 10)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Logarithms of all values

**Use Cases:**
- Base-10 logarithmic transformations
- Custom base calculations for specific models
- Data normalization and scaling
- When you need "log", "logarithm", or specific base calculations

**Example:**
```python
LOG(ctx, [100, 1000, 10000], base=10)  # Returns [Decimal('2'), Decimal('3'), Decimal('4')]
```

### ABS

**Purpose:** Calculate absolute value using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Absolute values of all input values

**Use Cases:**
- Calculate variance or deviation magnitudes
- Remove negative signs from differences
- Risk calculations requiring positive values
- When you need "absolute value", "magnitude", or "positive" values

**Example:**
```python
ABS(ctx, [-10, -5, 10, 15])  # Returns [Decimal('10'), Decimal('5'), Decimal('10'), Decimal('15')]
```

### SIGN

**Purpose:** Return sign of numbers (-1, 0, or 1).

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** list[int] - Signs of all input values (-1 for negative, 0 for zero, 1 for positive)

**Use Cases:**
- Determine direction of changes (positive/negative)
- Classify gains vs losses
- Conditional logic based on value signs
- When you need to identify "positive", "negative", or "direction" of values

**Example:**
```python
SIGN(ctx, [-15, 15, 0, -10, 20])  # Returns [-1, 1, 0, -1, 1]
```

### MOD

**Purpose:** Calculate remainder after division using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `dividends`: Series/array of dividend values (float, Decimal, Polars Series, NumPy array, or file path)
- `divisors`: Series/array of divisor values (same length as dividends, or single value, or file path)

**Returns:** list[Decimal] - Remainders of dividend % divisor for each pair

**Use Cases:**
- Calculate periodic patterns (monthly, quarterly cycles)
- Determine remainder amounts in allocations
- Modular arithmetic for financial calculations
- When you need "remainder", "modulo", or "cyclical" calculations

**Example:**
```python
MOD(ctx, [23, 10, 17], divisors=[5, 3, 4])  # Returns [Decimal('3'), Decimal('1'), Decimal('1')]
```

### ROUND

**Purpose:** Round numbers to specified digits using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers to round (float, Decimal, Polars Series, NumPy array, or file path)
- `num_digits`: Number of decimal places

**Returns:** list[Decimal] - Rounded numbers

**Use Cases:**
- Format financial reports to specific precision
- Round currency amounts to cents
- Standardize decimal places for presentation
- When you need to "round", "format", or "precision" control

**Example:**
```python
ROUND(ctx, [3.14159, 2.71828, 1.41421], num_digits=2)  # Returns [Decimal('3.14'), Decimal('2.72'), Decimal('1.41')]
```

### ROUNDUP

**Purpose:** Round numbers up using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers to round up (float, Decimal, Polars Series, NumPy array, or file path)
- `num_digits`: Number of decimal places

**Returns:** list[Decimal] - Rounded up numbers

**Use Cases:**
- Conservative estimates and budgeting
- Ceiling calculations for capacity planning
- Ensure minimum thresholds are met
- When you need "round up", "ceiling", or "conservative" estimates

**Example:**
```python
ROUNDUP(ctx, [3.14159, 2.71828, 1.41421], num_digits=2)  # Returns [Decimal('3.15'), Decimal('2.72'), Decimal('1.42')]
```

### ROUNDDOWN

**Purpose:** Round numbers down using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers to round down (float, Decimal, Polars Series, NumPy array, or file path)
- `num_digits`: Number of decimal places

**Returns:** list[Decimal] - Rounded down numbers

**Use Cases:**
- Conservative revenue projections
- Floor calculations for minimum values
- Ensure maximum limits are not exceeded
- When you need "round down", "floor", or "conservative" calculations

**Example:**
```python
ROUNDDOWN(ctx, [3.14159, 2.71828, 1.41421], num_digits=2)  # Returns [Decimal('3.14'), Decimal('2.71'), Decimal('1.41')]
```

### WEIGHTED_AVERAGE

**Purpose:** Calculate weighted average of values using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array of values (float, Decimal, Polars Series, NumPy array, or file path)
- `weights`: Array of weights (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Weighted average

**Use Cases:**
- Portfolio performance calculations
- Weighted cost of capital (WACC)
- Average prices weighted by volume
- When you need "weighted average", "portfolio", or "importance-weighted" calculations

**Example:**
```python
WEIGHTED_AVERAGE(ctx, [100, 200, 300], weights=[0.2, 0.3, 0.5])  # Returns Decimal('230')
```

### GEOMETRIC_MEAN

**Purpose:** Calculate geometric mean using Decimal precision (useful for growth rates).

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of positive numbers (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Geometric mean

**Use Cases:**
- Calculate average growth rates (CAGR)
- Portfolio return calculations
- Compound annual growth rate analysis
- When you need "geometric mean", "CAGR", "compound growth", or "average growth rate"

**Example:**
```python
GEOMETRIC_MEAN(ctx, [1.05, 1.08, 1.12, 1.03])  # Returns average growth multiplier
```

### HARMONIC_MEAN

**Purpose:** Calculate harmonic mean using Decimal precision (useful for rates/ratios).

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of positive numbers (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Harmonic mean

**Use Cases:**
- Average P/E ratios for portfolios
- Average interest rates
- Cost per unit calculations
- When you need "harmonic mean", "average rates", "P/E ratios", or "rate averaging"

**Example:**
```python
HARMONIC_MEAN(ctx, [15.2, 22.8, 18.5, 12.3])  # Returns average P/E ratio
```

### CUMSUM

**Purpose:** Calculate cumulative sum using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Array of cumulative sums

**Use Cases:**
- Running totals for cash flow analysis
- Cumulative revenue or expense tracking
- Year-to-date calculations
- When you need "cumulative", "running total", "YTD", or "progressive sum"

**Example:**
```python
CUMSUM(ctx, [10, 20, 30, 40])  # Returns [Decimal('10'), Decimal('30'), Decimal('60'), Decimal('100')]
```

### CUMPROD

**Purpose:** Calculate cumulative product using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Series/array of numbers (float, Decimal, Polars Series, NumPy array, or file path)
- `output_filename`: Optional filename to save results as parquet file

**Returns:** list[Decimal] - Array of cumulative products

**Use Cases:**
- Compound growth calculations
- Cumulative return analysis
- Progressive multiplication factors
- When you need "cumulative product", "compound", "progressive growth", or "cumulative returns"

**Example:**
```python
CUMPROD(ctx, [1.05, 1.08, 1.12])  # Returns [Decimal('1.05'), Decimal('1.134'), Decimal('1.269')]
```

### VARIANCE_WEIGHTED

**Purpose:** Calculate weighted variance using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `values`: Array of values (returns, prices, or other financial metrics) (float, Decimal, Polars Series, NumPy array, or file path)
- `weights`: Array of weights (portfolio weights, importance factors, etc.) (float, Decimal, Polars Series, NumPy array, or file path)

**Returns:** Decimal - Weighted variance

**Use Cases:**
- Portfolio risk analysis
- Weighted dispersion calculations
- Risk-weighted variance measurements
- When you need "weighted variance", "portfolio risk", "weighted dispersion", or "risk analysis"

**Example:**
```python
VARIANCE_WEIGHTED(ctx, [0.12, 0.08, 0.15, 0.06], weights=[0.4, 0.3, 0.2, 0.1])  # Returns portfolio variance
```

## 2. Conditional Aggregation and Counting

### COUNTIF

**Purpose:** Count cells that meet one condition.

**Parameters:**
- `ctx`: RunContext object for file operations
- `range_to_evaluate`: Range of values to evaluate (list, Polars Series, NumPy array, or file path)
- `criteria`: Criteria to match (supports operators: >, <, >=, <=, =, <>, wildcards *, ?)

**Returns:** int - Count of cells meeting the criteria

**Use Cases:**
- Count high-value transactions for risk analysis
- Identify outliers or specific segments in data
- Count transactions above/below thresholds
- When you need to "count", "tally", or "enumerate" based on conditions

**Example:**
```python
COUNTIF(ctx, [100, 200, 150, 300, 50], criteria=">150")  # Returns 2
COUNTIF(ctx, ["Sales", "Marketing", "Sales", "IT"], criteria="Sales")  # Returns 2
```

### COUNTIFS

**Purpose:** Count cells that meet multiple conditions across different ranges.

**Parameters:**
- `ctx`: RunContext object for file operations
- `criteria_ranges`: List of ranges to evaluate (list, Polars Series, NumPy array, or file paths)
- `criteria_values`: List of criteria corresponding to each range

**Returns:** int - Count of rows where all criteria are met

**Use Cases:**
- Count high-value sales in specific regions for territory analysis
- Multi-dimensional data analysis and segmentation
- Complex filtering with multiple conditions
- When you need to count based on "multiple criteria", "AND conditions", or "intersecting filters"

**Example:**
```python
amounts = [100, 200, 150, 300, 50]
categories = ["A", "B", "A", "A", "B"]
COUNTIFS(ctx, [amounts, categories], criteria_values=[">100", "A"])  # Returns 2
```

### SUMIF

**Purpose:** Sum numbers that meet one condition using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `range_to_evaluate`: Range to evaluate against criteria (list, Polars Series, NumPy array, or file path)
- `criteria`: Criteria to match (supports operators: >, <, >=, <=, =, <>, wildcards *, ?)
- `sum_range`: Range to sum (optional, defaults to range_to_evaluate, can be file path)

**Returns:** Decimal - Sum of values meeting the criteria

**Use Cases:**
- Sum revenue for specific regions or categories
- Calculate conditional totals for financial analysis
- Aggregate expenses above budget thresholds
- When you need "conditional sum", "filtered total", or "sum where" calculations

**Example:**
```python
categories = ["A", "B", "A", "C", "A"]
values = [100, 200, 150, 300, 50]
SUMIF(ctx, categories, criteria="A", sum_range=values)  # Returns Decimal('300')
```

### SUMIFS

**Purpose:** Sum numbers that meet multiple conditions using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `sum_range`: Range of values to sum (list, Polars Series, NumPy array, or file path)
- `criteria_ranges`: List of ranges to evaluate (must all be same length as sum_range)
- `criteria_values`: List of criteria corresponding to each range

**Returns:** Decimal - Sum of values where all criteria are met

**Use Cases:**
- Multi-dimensional revenue analysis
- Complex cost allocation and summation
- Sum values based on multiple filtering criteria
- When you need "conditional sum with multiple criteria", "multi-filter aggregation", or "complex summation"

**Example:**
```python
amounts = [100, 200, 150, 300, 50]
categories = ["A", "B", "A", "A", "B"]
regions = ["North", "South", "North", "West", "South"]
SUMIFS(ctx, amounts, criteria_ranges=[categories, regions], criteria_values=["A", "North"])  # Returns Decimal('250')
```

### AVERAGEIF

**Purpose:** Calculate average of cells that meet one condition using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `range_to_evaluate`: Range to evaluate against criteria (list, Polars Series, NumPy array, or file path)
- `criteria`: Criteria to match (supports operators: >, <, >=, <=, =, <>, wildcards *, ?)
- `average_range`: Range to average (optional, defaults to range_to_evaluate, can be file path)

**Returns:** Decimal - Average of values meeting the criteria

**Use Cases:**
- Average transaction size by customer segment
- Calculate conditional averages for performance analysis
- Average performance metrics for specific groups
- When you need "conditional average", "filtered mean", or "average where" calculations

**Example:**
```python
categories = ["A", "B", "A", "C", "A"]
values = [100, 200, 150, 300, 50]
AVERAGEIF(ctx, categories, criteria="A", average_range=values)  # Returns Decimal('100')
```

### AVERAGEIFS

**Purpose:** Calculate average of cells that meet multiple conditions using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `average_range`: Range of values to average (list, Polars Series, NumPy array, or file path)
- `criteria_ranges`: List of ranges to evaluate (must all be same length as average_range)
- `criteria_values`: List of criteria corresponding to each range

**Returns:** Decimal - Average of values where all criteria are met

**Use Cases:**
- Multi-dimensional performance analysis
- Average returns across multiple factors
- Complex conditional averaging for financial metrics
- When you need "conditional average with multiple criteria", "multi-filter averaging", or "sophisticated mean calculations"

**Example:**
```python
amounts = [100, 200, 150, 300, 50]
categories = ["A", "B", "A", "A", "B"]
regions = ["North", "South", "North", "West", "South"]
AVERAGEIFS(ctx, amounts, criteria_ranges=[categories, regions], criteria_values=["A", "North"])  # Returns Decimal('125')
```

### MAXIFS

**Purpose:** Find maximum value based on multiple criteria using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `max_range`: Range of values to find maximum from (list, Polars Series, NumPy array, or file path)
- `criteria_ranges`: List of ranges to evaluate (must all be same length as max_range)
- `criteria_values`: List of criteria corresponding to each range

**Returns:** Decimal - Maximum value where all criteria are met

**Use Cases:**
- Find peak performance metrics in specific segments
- Identify highest values with multiple conditions
- Maximum revenue analysis by region and product
- When you need "conditional maximum", "filtered peak", or "max where multiple conditions"

**Example:**
```python
amounts = [100, 200, 150, 300, 50]
categories = ["A", "B", "A", "A", "B"]
regions = ["North", "South", "North", "West", "South"]
MAXIFS(ctx, amounts, criteria_ranges=[categories, regions], criteria_values=["A", "North"])  # Returns Decimal('150')
```

### MINIFS

**Purpose:** Find minimum value based on multiple criteria using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `min_range`: Range of values to find minimum from (list, Polars Series, NumPy array, or file path)
- `criteria_ranges`: List of ranges to evaluate (must all be same length as min_range)
- `criteria_values`: List of criteria corresponding to each range

**Returns:** Decimal - Minimum value where all criteria are met

**Use Cases:**
- Find lowest costs in specific segments
- Identify minimum performance thresholds
- Minimum value analysis with multiple conditions
- When you need "conditional minimum", "filtered floor", or "min where multiple conditions"

**Example:**
```python
amounts = [100, 200, 150, 300, 50]
categories = ["A", "B", "A", "A", "B"]
regions = ["North", "South", "North", "West", "South"]
MINIFS(ctx, amounts, criteria_ranges=[categories, regions], criteria_values=["A", "North"])  # Returns Decimal('100')
```

### SUMPRODUCT

**Purpose:** Sum the products of corresponding ranges using Decimal precision.

**Parameters:**
- `ctx`: RunContext object for file operations
- `*ranges`: Two or more ranges of equal length to multiply and sum

**Returns:** Decimal - Sum of products of corresponding elements

**Use Cases:**
- Portfolio value calculations (quantities × prices)
- Revenue calculations (units × unit prices)
- Weighted sum calculations
- When you need "multiply and sum", "portfolio calculations", or "weighted aggregations"

**Example:**
```python
SUMPRODUCT(ctx, [1, 2, 3], [4, 5, 6])  # Returns Decimal('32') = (1×4) + (2×5) + (3×6)
SUMPRODUCT(ctx, [10, 20], [5, 3], [2, 1])  # Returns Decimal('160') = (10×5×2) + (20×3×1)
```

### COUNTBLANK

**Purpose:** Count blank/empty cells in a range.

**Parameters:**
- `ctx`: RunContext object for file operations
- `range_to_evaluate`: Range to evaluate for blank/null values (list, Polars Series, NumPy array, or file path)

**Returns:** int - Count of blank/null cells

**Use Cases:**
- Data quality assessment and completeness analysis
- Identify missing data in financial datasets
- Count incomplete records for reporting
- When you need to assess "data completeness", "missing values", or "data quality"

**Example:**
```python
COUNTBLANK(ctx, [1, None, 3, "", 5, None])  # Returns 3
COUNTBLANK(ctx, ["A", "B", "", None, "C"])  # Returns 2
```

### COUNTA

**Purpose:** Count non-empty cells in a range.

**Parameters:**
- `ctx`: RunContext object for file operations
- `range_to_evaluate`: Range to evaluate for non-empty values (list, Polars Series, NumPy array, or file path)

**Returns:** int - Count of non-empty cells

**Use Cases:**
- Determine actual dataset size excluding missing values
- Count valid transaction records
- Data completeness analysis
- When you need to count "valid records", "non-missing data", or "actual data points"

**Example:**
```python
COUNTA(ctx, [1, None, 3, "", 5, 0])  # Returns 4 (counts 1, 3, 5, 0)
COUNTA(ctx, ["A", "B", "", None, "C"])  # Returns 3 (counts "A", "B", "C")
```

### AGGREGATE

**Purpose:** Perform various aggregations with error handling and filtering using Decimal precision.

**Parameters:**
- `function_num`: Function number (1=AVERAGE, 2=COUNT, 3=COUNTA, 4=MAX, 5=MIN, 6=PRODUCT, 9=SUM, 12=MEDIAN, 14=LARGE, 15=SMALL)
- `options`: Options for handling errors and filtering (0=default, 2=ignore errors, 3=ignore hidden and errors)
- `array`: Array to aggregate
- `k`: Additional parameter for functions like LARGE, SMALL, PERCENTILE

**Returns:** Decimal - Aggregated result

**Use Cases:**
- Robust financial calculations ignoring errors
- Clean aggregation of messy financial data
- Error-resistant analysis for data with inconsistencies
- When you need "error-resistant calculations", "robust aggregation", or "clean financial analysis"

**Example:**
```python
AGGREGATE(9, options=2, array=[10, "Error", 20, 30])  # Returns Decimal('60') (SUM ignoring errors)
AGGREGATE(4, options=0, array=[10, 20, 30, 40])  # Returns Decimal('40') (MAX)
```

### SUBTOTAL

**Purpose:** Calculate subtotals with filtering capability using Decimal precision.

**Parameters:**
- `function_num`: Function number (101-111 ignore hidden values, 1-11 include all)
- `ref1`: Reference range to calculate subtotal for

**Returns:** Decimal - Subtotal result

**Use Cases:**
- Financial reporting with filtered data
- Subtotals that ignore hidden rows
- Hierarchical financial analysis
- When you need "filtered subtotals", "visible data aggregation", or "reporting subtotals"

**Example:**
```python
SUBTOTAL(109, ref1=[10, 20, 30, 40])  # Returns Decimal('100') (SUM)
SUBTOTAL(101, ref1=[10, 20, 30, 40])  # Returns Decimal('25') (AVERAGE)


## 3. Conditional Logic

### MULTI_CONDITION_LOGIC

**Purpose:** Apply complex multi-condition logic with nested if/elif/else structures for hierarchical business rule implementation.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to apply logic to, or file path to load data from
- `condition_tree`: Nested dictionary defining conditional logic structure
- `output_filename`: Optional filename to save results as parquet file

**Returns:** pl.DataFrame or Path - DataFrame with conditional results column, or path if output_filename provided

**Use Cases:**
- Credit risk assessment with multiple criteria
- Investment allocation strategies based on age and risk tolerance
- Revenue categorization for financial reporting
- Complex business rule implementation
- When you need "hierarchical conditions", "nested business rules", "multi-tier classification", or "complex decision trees"

**Example:**
```python
risk_tree = {
    'if': 'credit_score >= 750',
    'then': 'Low Risk',
    'elif': [
        {'condition': 'credit_score >= 650', 'then': 'Medium Risk'},
        {'condition': 'debt_ratio <= 0.3', 'then': 'Medium Risk'}
    ],
    'else': 'High Risk'
}
MULTI_CONDITION_LOGIC(ctx, customer_df, condition_tree=risk_tree)
```

### NESTED_IF_LOGIC

**Purpose:** Handle nested conditional statements with cascading if-then-else logic for complex financial decision trees.

**Parameters:**
- `run_context`: RunContext object for file operations
- `conditions_list`: List of condition strings or Polars expressions
- `results_list`: List of corresponding results for each condition
- `default_value`: Default value if no conditions are met
- `df_context`: Optional DataFrame context for string condition evaluation
- `output_filename`: Optional filename to save results as parquet file

**Returns:** List, DataFrame, or Path - Conditional results or path if output_filename provided

**Use Cases:**
- Bond rating classification systems
- Commission tier calculations based on sales performance
- Investment risk categorization
- Customer segmentation with multiple criteria
- Performance bonus calculations
- When you need "cascading conditions", "tiered classification", "sequential evaluation", or "priority-based logic"

**Example:**
```python
conditions = ['credit_score >= 800', 'credit_score >= 700', 'credit_score >= 600']
ratings = ['AAA', 'AA', 'A']
NESTED_IF_LOGIC(ctx, conditions, results_list=ratings, default_value='BBB', df_context=bond_df)
```

### CASE_WHEN

**Purpose:** SQL-style CASE WHEN logic for multiple conditional branches with clean and efficient conditional processing.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to apply case logic to, or file path to load data from
- `case_conditions`: List of dictionaries with 'when' and 'then' keys
- `output_filename`: Optional filename to save results as parquet file

**Returns:** pl.DataFrame or Path - DataFrame with case results column, or path if output_filename provided

**Use Cases:**
- Customer segment classification based on revenue
- Performance rating systems
- Investment allocation based on multiple factors
- Product categorization for financial analysis
- When you need "SQL-style conditions", "multiple branches", "case statements", or "conditional classification"

**Example:**
```python
segments = [
    {'when': 'annual_revenue >= 1000000', 'then': 'Enterprise'},
    {'when': 'annual_revenue >= 100000', 'then': 'Corporate'},
    {'when': 'annual_revenue >= 10000', 'then': 'SMB'},
    {'else': 'Startup'}
]
CASE_WHEN(ctx, customer_df, case_conditions=segments)
```

### CONDITIONAL_AGGREGATION

**Purpose:** Aggregate data based on conditions, similar to SQL HAVING clause, for sophisticated financial analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to aggregate, or file path to load data from
- `group_columns`: List of columns to group by
- `condition`: Condition string to filter data before aggregation
- `aggregation_func`: Aggregation function ('sum', 'count', 'mean', 'max', 'min', 'std', 'var')
- `target_column`: Column to aggregate (required for most functions except 'count')
- `output_filename`: Optional filename to save results as parquet file

**Returns:** pl.DataFrame or Path - DataFrame with conditional aggregations, or path if output_filename provided

**Use Cases:**
- Sum high-value transactions by region
- Count profitable customers by segment
- Average revenue for enterprise clients by quarter
- Maximum deal size analysis for large deals
- Conditional financial metrics calculation
- When you need "conditional aggregation", "filtered summation", "conditional metrics", or "business rule aggregation"

**Example:**
```python
CONDITIONAL_AGGREGATION(
    ctx, sales_df,
    group_columns=['region'],
    condition='amount > 1000',
    aggregation_func='sum',
    target_column='amount'
)
```

## 4. Data Cleaning Operations

### STANDARDIZE_CURRENCY

**Purpose:** Standardize currency formats for financial data consistency across different systems and reporting requirements.

**Parameters:**
- `run_context`: RunContext object for file operations
- `currency_series`: Series of currency values in various formats (List, Polars Series, or file path)
- `target_format`: Target currency format (e.g., 'USD', 'EUR', 'GBP', 'JPY')
- `output_filename`: Filename to save standardized currency results

**Returns:** Path - Path to the saved parquet file containing standardized currency values

**Use Cases:**
- Multi-currency financial reporting consolidation
- Regulatory compliance for international transactions
- Data preparation for financial analysis and modeling
- Standardization for accounting system integration
- When you need to "standardize currency", "normalize money formats", "consolidate currencies", or "clean financial data"

**Example:**
```python
STANDARDIZE_CURRENCY(ctx, ["$1,234.56", "USD 1234.56", "1234.56 USD"], target_format="USD", output_filename="std_currency.parquet")
```

### CLEAN_NUMERIC

**Purpose:** Clean numeric data by removing non-numeric characters and converting to proper numeric format for mathematical operations.

**Parameters:**
- `run_context`: RunContext object for file operations
- `mixed_series`: Series with mixed data containing numbers and non-numeric characters (List, Polars Series, or file path)
- `output_filename`: Filename to save cleaned numeric results

**Returns:** Path - Path to the saved parquet file containing cleaned numeric values

**Use Cases:**
- Preparing imported financial data for analysis
- Cleaning data from various accounting systems
- Standardizing numeric formats across data sources
- Converting text-based financial reports to numeric format
- When you need to "clean numbers", "extract numeric values", "remove formatting", or "prepare data for calculations"

**Example:**
```python
CLEAN_NUMERIC(ctx, ["$1,234.56", "€987.65", "(500.00)", "2,345.67%"], output_filename="clean_numbers.parquet")
```

### NORMALIZE_NAMES

**Purpose:** Normalize company/customer names for consistent identification and reporting across financial systems.

**Parameters:**
- `run_context`: RunContext object for file operations
- `name_series`: Series of names to normalize (List, Polars Series, or file path)
- `normalization_rules`: Dictionary mapping variations to standard forms
- `output_filename`: Filename to save normalized names

**Returns:** Path - Path to the saved parquet file containing normalized names

**Use Cases:**
- Customer data deduplication and master data management
- Regulatory reporting with standardized entity names
- Financial consolidation across subsidiaries
- Vendor management and procurement standardization
- When you need to "standardize names", "normalize entities", "clean company names", or "deduplicate customers"

**Example:**
```python
rules = {"incorporated": "Inc.", "corporation": "Corp.", "company": "Co."}
NORMALIZE_NAMES(ctx, ["Apple Inc.", "Apple Incorporated", "APPLE INC"], normalization_rules=rules, output_filename="std_companies.parquet")
```

### REMOVE_DUPLICATES

**Purpose:** Remove duplicate records with configurable options for financial data integrity and accurate reporting.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to process (Polars DataFrame or file path)
- `subset_columns`: List of column names to check for duplicates
- `keep_method`: Method to keep records ('first', 'last', 'none')
- `output_filename`: Filename to save deduplicated results

**Returns:** Path - Path to the saved parquet file containing deduplicated data

**Use Cases:**
- Transaction deduplication in payment processing
- Customer master data management
- Financial report accuracy and compliance
- Data warehouse ETL processes
- When you need to "remove duplicates", "deduplicate data", "clean records", or "ensure data integrity"

**Example:**
```python
REMOVE_DUPLICATES(ctx, transactions_df, subset_columns=["transaction_id"], keep_method="first", output_filename="unique_transactions.parquet")
```

### STANDARDIZE_DATES

**Purpose:** Convert various date formats to a standardized format for consistent financial reporting and time-series analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `date_series`: Series of dates in various formats (List, Polars Series, or file path)
- `target_format`: Target date format string (e.g., '%Y-%m-%d', '%m/%d/%Y')
- `output_filename`: Filename to save standardized dates

**Returns:** Path - Path to the saved parquet file containing standardized dates

**Use Cases:**
- Financial report standardization across systems
- Regulatory compliance for date formatting requirements
- Data warehouse ETL date normalization
- Cross-border financial data integration
- When you need to "standardize dates", "normalize date formats", "clean date data", or "prepare time series data"

**Example:**
```python
STANDARDIZE_DATES(ctx, ["01/15/2023", "2023-01-15", "15-Jan-2023"], target_format="%Y-%m-%d", output_filename="std_dates.parquet")
```

## 5. Data Comparison and Ranking

### RANK_BY_COLUMN

**Purpose:** Rank records by column values with financial precision for performance analysis and competitive benchmarking.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to rank (Polars DataFrame or file path)
- `column`: Column to rank by
- `ascending`: Sort order (default False for descending - highest values get rank 1)
- `method`: Ranking method ('average', 'min', 'max', 'dense', 'ordinal')
- `output_filename`: Filename to save results as parquet file

**Returns:** Path - Path to saved results file with ranking column added

**Use Cases:**
- Ranking investment portfolios by annual return for performance evaluation
- Customer ranking by total revenue for account prioritization
- Product profitability rankings for strategic decisions
- Employee performance rankings for compensation planning
- When you need to "rank", "order", "prioritize", or "compare performance"

### PERCENTILE_RANK

**Purpose:** Calculate percentile rank for each value to understand relative performance and position within datasets.

**Parameters:**
- `run_context`: RunContext object for file operations
- `series`: Series to rank (Polars Series, list, NumPy array, or file path)
- `method`: Ranking method for ties ('average', 'min', 'max', 'dense', 'ordinal')
- `output_filename`: Filename to save results as parquet file

**Returns:** Path - Path to saved results file containing percentile ranks (0-100)

**Use Cases:**
- Risk assessment showing what percentage of returns fall below each value
- Credit score analysis relative to portfolio distribution
- Performance benchmarking against peer groups
- Outlier detection and threshold analysis
- When you need "percentile analysis", "relative position", "benchmarking", or "distribution analysis"

### COMPARE_PERIODS

**Purpose:** Compare financial values between specified periods with comprehensive variance analysis for period-over-period reporting.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame containing period data (Polars DataFrame or file path)
- `value_column`: Column containing values to compare
- `period_column`: Column containing period identifiers
- `periods_to_compare`: List of exactly 2 period identifiers to compare
- `output_filename`: Filename to save results as parquet file

**Returns:** Path - Path to saved results file containing period comparison with variance metrics

**Use Cases:**
- Year-over-year revenue analysis for growth tracking
- Quarterly expense comparisons for budget management
- Monthly performance variance reporting
- Period-over-period profitability analysis
- When you need "period comparison", "variance analysis", "growth tracking", or "trend analysis"

### VARIANCE_FROM_TARGET

**Purpose:** Calculate variance from target values with comprehensive analysis for budget control and performance management.

**Parameters:**
- `run_context`: RunContext object for file operations
- `actual_values`: Actual values achieved (Series, list, NumPy array, or file path)
- `target_values`: Target/budget values (Series, list, NumPy array, or file path)
- `output_filename`: Filename to save results as parquet file

**Returns:** Path - Path to saved results file containing variance analysis with absolute and percentage variances

**Use Cases:**
- Budget variance analysis for financial control
- Sales performance vs targets for commission calculations
- Expense variance reporting for cost management
- KPI tracking against strategic objectives
- When you need "budget variance", "target analysis", "performance tracking", or "variance reporting"

### RANK_CORRELATION

**Purpose:** Calculate Spearman rank correlation coefficient to measure monotonic relationships between financial variables.

**Parameters:**
- `run_context`: RunContext object for file operations
- `series1`: First series for correlation (Series, list, NumPy array, or file path)
- `series2`: Second series for correlation (Series, list, NumPy array, or file path)

**Returns:** float - Spearman rank correlation coefficient (-1 to 1)

**Use Cases:**
- Analyzing relationship between credit scores and default rates
- Investment performance correlation across different periods
- Company size vs profitability relationship analysis
- Risk factor correlation for portfolio management
- When you need "correlation analysis", "relationship measurement", "monotonic relationships", or "rank correlation"

## 6. Data Filtering and Selection

### FILTER_BY_DATE_RANGE

**Purpose:** Filter DataFrame rows by date range using optimized date operations for time-based financial analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to filter (Polars DataFrame or file path)
- `date_column`: Name of the date column to filter on
- `start_date`: Start date in ISO format (YYYY-MM-DD)
- `end_date`: End date in ISO format (YYYY-MM-DD)
- `output_filename`: Filename to save filtered results as parquet file

**Returns:** Path - Path to saved filtered DataFrame

**Use Cases:**
- Filter transactions for quarterly or annual reporting periods
- Extract data for specific fiscal years or quarters
- Analyze performance within date ranges for trend analysis
- Regulatory reporting requiring specific time periods
- When you need to "filter by date", "time period analysis", "date range selection", or "period-specific data"

**Example:**
```python
FILTER_BY_DATE_RANGE(ctx, transactions_df, date_column='transaction_date', start_date='2024-01-01', end_date='2024-12-31', output_filename='yearly_transactions.parquet')
```

### FILTER_BY_VALUE

**Purpose:** Filter DataFrame rows based on column values using comparison operators for targeted data analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to filter (Polars DataFrame or file path)
- `column`: Column name to filter on
- `operator`: Comparison operator ('>', '<', '>=', '<=', '==', '!=')
- `value`: Value to compare against
- `output_filename`: Filename to save filtered results as parquet file

**Returns:** Path - Path to saved filtered DataFrame

**Use Cases:**
- Filter high-value transactions for risk analysis
- Extract customers above revenue thresholds
- Identify outliers or exceptional performance
- Filter data for specific value ranges or criteria
- When you need to "filter by value", "threshold analysis", "conditional selection", or "value-based filtering"

**Example:**
```python
FILTER_BY_VALUE(ctx, sales_df, column='amount', operator='>', value=1000, output_filename='high_value_sales.parquet')
```

### FILTER_BY_MULTIPLE_CONDITIONS

**Purpose:** Filter DataFrame using multiple conditions with AND logic for complex data selection criteria.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to filter (Polars DataFrame or file path)
- `conditions_dict`: Dictionary of conditions {column: value} or {column: 'operator:value'}
- `output_filename`: Filename to save filtered results as parquet file

**Returns:** Path - Path to saved filtered DataFrame

**Use Cases:**
- Multi-criteria customer segmentation analysis
- Complex risk assessment with multiple factors
- Advanced filtering for regulatory compliance
- Sophisticated data selection for modeling
- When you need "multiple conditions", "complex filtering", "AND logic", or "multi-criteria selection"

**Example:**
```python
FILTER_BY_MULTIPLE_CONDITIONS(ctx, df, conditions_dict={'region': 'North', 'sales': '>:1000', 'status': 'active'}, output_filename='filtered_data.parquet')
```

### TOP_N

**Purpose:** Select top N records by value using optimized ranking operations for performance analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to select from (Polars DataFrame or file path)
- `column`: Column to sort by for ranking
- `n`: Number of top records to select
- `ascending`: Sort order (False for top values, True for bottom values)
- `output_filename`: Filename to save selected results as parquet file

**Returns:** Path - Path to saved DataFrame with top N records

**Use Cases:**
- Identify top-performing customers, products, or regions
- Select highest revenue transactions for analysis
- Find best-performing investments or portfolios
- Extract top performers for benchmarking
- When you need "top performers", "highest values", "best results", or "ranking analysis"

**Example:**
```python
TOP_N(ctx, customers_df, column='revenue', n=10, ascending=False, output_filename='top_customers.parquet')
```

### BOTTOM_N

**Purpose:** Select bottom N records by value using optimized ranking for identifying underperformers.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to select from (Polars DataFrame or file path)
- `column`: Column to sort by for ranking
- `n`: Number of bottom records to select
- `output_filename`: Filename to save selected results as parquet file

**Returns:** Path - Path to saved DataFrame with bottom N records

**Use Cases:**
- Identify underperforming customers or products
- Find lowest margin transactions for cost analysis
- Extract worst-performing investments for review
- Analyze bottom performers for improvement strategies
- When you need "underperformers", "lowest values", "worst results", or "bottom ranking"

**Example:**
```python
BOTTOM_N(ctx, products_df, column='profit_margin', n=5, output_filename='lowest_margin_products.parquet')
```

### SAMPLE_DATA

**Purpose:** Extract random samples from large datasets using optimized sampling for statistical analysis.

**Parameters:**
- `run_context`: RunContext object for file operations
- `df`: DataFrame to sample from (Polars DataFrame or file path)
- `n_samples`: Number of random samples to extract
- `random_state`: Random seed for reproducible sampling (optional)
- `output_filename`: Filename to save sampled results as parquet file

**Returns:** Path - Path to saved DataFrame with sampled records

**Use Cases:**
- Create representative samples for statistical analysis
- Reduce large datasets for faster processing
- Generate test datasets for model validation
- Random sampling for audit or quality control
- When you need "random sampling", "statistical sampling", "data reduction", or "representative subset"

**Example:**
```python
SAMPLE_DATA(ctx, large_dataset_df, n_samples=1000, random_state=42, output_filename='sample_data.parquet')
```

